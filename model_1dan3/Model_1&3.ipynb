{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYLGw_RO7Z_X"
      },
      "source": [
        "## Prepare the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Model\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "metadata": {
        "id": "A1R8aMSFzDYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the archive\n",
        "zip_ref = zipfile.ZipFile(\"/content/archive.zip\", 'r')\n",
        "zip_ref.extractall(\"tmp/\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "WPQGiFskIvBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_mappings = {\n",
        "    '/content/tmp/Dataset': '/content/tmp/soil_type',\n",
        "    '/content/tmp/soil_type/Train': '/content/tmp/soil_type/train',\n",
        "    '/content/tmp/soil_type/test': '/content/tmp/soil_type/validation',\n",
        "    '/content/tmp/soil_type/train/Alluvial soil': '/content/tmp/soil_type/train/alluvial',\n",
        "    '/content/tmp/soil_type/train/Black Soil' : '/content/tmp/soil_type/train/black',\n",
        "    '/content/tmp/soil_type/train/Clay soil' : '/content/tmp/soil_type/train/clay',\n",
        "    '/content/tmp/soil_type/train/Red soil' : '/content/tmp/soil_type/train/red',\n",
        "    '/content/tmp/soil_type/validation/Alluvial soil': '/content/tmp/soil_type/validation/alluvial',\n",
        "    '/content/tmp/soil_type/validation/Black Soil' : '/content/tmp/soil_type/validation/black',\n",
        "    '/content/tmp/soil_type/validation/Clay soil' : '/content/tmp/soil_type/validation/clay',\n",
        "    '/content/tmp/soil_type/validation/Red soil' : '/content/tmp/soil_type/validation/red',\n",
        "}\n",
        "\n",
        "# Changing the folder name\n",
        "for old_folder_path, new_folder_path in folder_mappings.items():\n",
        "    if os.path.exists(new_folder_path):\n",
        "        shutil.rmtree(new_folder_path)  # Delete the destination folder if it already exists and is not empty\n",
        "    os.rename(old_folder_path, new_folder_path)"
      ],
      "metadata": {
        "id": "UljKSDBaI4kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of directories and prefixes to be used\n",
        "directories = ['/content/tmp/soil_type/train/alluvial',\n",
        "               '/content/tmp/soil_type/train/black',\n",
        "               '/content/tmp/soil_type/train/clay',\n",
        "               '/content/tmp/soil_type/train/red',\n",
        "               '/content/tmp/soil_type/validation/alluvial',\n",
        "               '/content/tmp/soil_type/validation/black',\n",
        "               '/content/tmp/soil_type/validation/clay',\n",
        "               '/content/tmp/soil_type/validation/red']\n",
        "\n",
        "prefixes = ['alluvial_', 'black_', 'clay_', 'red_', 'alluvial_', 'black_', 'clay_', 'red_']\n",
        "\n",
        "# Performing file renaming for each directory and prefix\n",
        "for directory, prefix in zip(directories, prefixes):\n",
        "    os.chdir(directory)\n",
        "    files = os.listdir()\n",
        "    for i, file in enumerate(files):\n",
        "        new_name = prefix + str(i+1) + '.jpg'\n",
        "        os.rename(file, new_name)"
      ],
      "metadata": {
        "id": "9wVvlZYLRD3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOV8jON3c3Jv"
      },
      "outputs": [],
      "source": [
        "# Define our example directories and files\n",
        "base_dir = '/content/tmp/soil_type'\n",
        "\n",
        "train_dir = os.path.join( base_dir, 'train')\n",
        "validation_dir = os.path.join( base_dir, 'validation')\n",
        "\n",
        "# Directory with training pictures\n",
        "train_alluvial_dir = os.path.join(train_dir, 'alluvial')\n",
        "train_black_dir = os.path.join(train_dir, 'black')\n",
        "train_clay_dir = os.path.join(train_dir, 'clay')\n",
        "train_red_dir = os.path.join(train_dir, 'red')\n",
        "\n",
        "# Directory with validation pictures\n",
        "validation_alluvial_dir = os.path.join(validation_dir, 'alluvial')\n",
        "validation_black_dir = os.path.join(train_dir, 'black')\n",
        "validation_clay_dir = os.path.join(train_dir, 'clay')\n",
        "validation_red_dir = os.path.join(train_dir, 'red')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample alluvial soil image:\")\n",
        "plt.imshow(load_img(f\"{os.path.join(train_alluvial_dir, os.listdir(train_alluvial_dir)[1])}\"))\n",
        "plt.show()\n",
        "\n",
        "# Load the first example of a alluvial soil\n",
        "sample_image  = load_img(f\"{os.path.join(train_alluvial_dir, os.listdir(train_alluvial_dir)[0])}\")\n",
        "# Convert the image into its numpy array representation\n",
        "sample_array = img_to_array(sample_image)\n",
        "print(f\"Each image has shape: {sample_array.shape}\")\n",
        "\n",
        "print('total training alluvial images :', len(os.listdir(      train_alluvial_dir ) ))\n",
        "print('total training black images :', len(os.listdir(      train_black_dir ) ))\n",
        "print('total training clay images :', len(os.listdir(      train_clay_dir ) ))\n",
        "print('total training red images :', len(os.listdir(      train_red_dir ) ))\n",
        "\n",
        "print('total validation alluvial images :', len(os.listdir( validation_alluvial_dir ) ))\n",
        "print('total validation black images :', len(os.listdir(      validation_black_dir ) ))\n",
        "print('total validation clay images :', len(os.listdir(      validation_clay_dir ) ))\n",
        "print('total validation red images :', len(os.listdir(      validation_red_dir ) ))"
      ],
      "metadata": {
        "id": "ckiEVVr93PtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training, Validation, and Test Generator"
      ],
      "metadata": {
        "id": "lTzbpwq9mkaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
        "  # Instantiate the ImageDataGenerator class\n",
        "  # Normalize pixel values and set arguments to augment the images\n",
        "  train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                     rotation_range=45,\n",
        "                                     width_shift_range=0.2,\n",
        "                                     height_shift_range=0.2,\n",
        "                                     shear_range=0.2,\n",
        "                                     zoom_range=0.2,\n",
        "                                     horizontal_flip=True,\n",
        "                                     fill_mode='nearest')\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  train_generator = train_datagen.flow_from_directory(directory=train_dir,\n",
        "                                                      batch_size=20,\n",
        "                                                      class_mode='categorical',\n",
        "                                                      target_size=(260, 260))\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class and set the rescale argument\n",
        "  # Validation data should not be augmented\n",
        "  validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  validation_generator = validation_datagen.flow_from_directory(directory=validation_dir,\n",
        "                                                                batch_size=20,\n",
        "                                                                class_mode='categorical',\n",
        "                                                                target_size=(260, 260))\n",
        "  return train_generator, validation_generator"
      ],
      "metadata": {
        "id": "MtiOfpyEYPLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test generators\n",
        "train_generator, validation_generator = train_val_generators(train_dir, validation_dir)\n",
        "\n",
        "# Fetching class names from the train generator\n",
        "class_names_train = list(train_generator.class_indices.keys())\n",
        "print(\"Class names train:\", class_names_train)\n",
        "\n",
        "# Fetching class names from the validation generator\n",
        "class_names_validation = list(validation_generator.class_indices.keys())\n",
        "print(\"Class names validation:\", class_names_validation)\n"
      ],
      "metadata": {
        "id": "Hb4Sc0kEZPAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "b2p00yOGm_Fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Model initialization\n",
        "model = Sequential()\n",
        "\n",
        "# First convolutional layer\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(260, 260, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second convolutional layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Third convolutional layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Displaying model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "dS-6thc_4v2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluated Model"
      ],
      "metadata": {
        "id": "ZF8QcmTOnHor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Callback class that stops training once accuracy reaches 95%\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.95):\n",
        "      print(\"\\nReached 95% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True"
      ],
      "metadata": {
        "id": "YOcwTWTskTgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate value\n",
        "learning_rate = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "OCZv4EOv55ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing custom callback\n",
        "callbacks = myCallback()\n",
        "steps_per_epoch = 1222 // 20\n",
        "validation_steps = 341 // 20\n",
        "#Training the model\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=100,\n",
        "                    verbose = 2,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=validation_steps,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "1TxWm1WI6B2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwcB2bPj7lIx"
      },
      "source": [
        "## Evaluate the results\n",
        "\n",
        "You will use the same code to plot the results. As you can see, the validation accuracy is also trending upwards as your training accuracy improves. This is a good sign that your model is no longer overfitting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2Fp6Se9rKuL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the model results\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining the train matrix\n",
        "train_data = train_generator.next()[0]\n",
        "\n",
        "# Calculating the number of samples and classes\n",
        "num_samples = train_data.shape[0]\n",
        "num_classes = train_data.shape[3]\n",
        "\n",
        "# Displaying the train matrix\n",
        "fig, axes = plt.subplots(num_classes, num_samples, figsize=(num_samples, num_classes))\n",
        "\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_samples):\n",
        "        axes[i][j].imshow(train_data[j, :, :, i])\n",
        "        axes[i][j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ebm672I3JQqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load the validation dataset\n",
        "labels = validation_generator.class_indices.keys()\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,  # Path to test directory\n",
        "    target_size=(260, 260),\n",
        "    batch_size=20,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_true = test_generator.classes\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "confusion_mtx = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot the confusion matrix as an image\n",
        "plt.imshow(confusion_mtx, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(labels))\n",
        "plt.xticks(tick_marks, labels, rotation=45)\n",
        "plt.yticks(tick_marks, labels)\n",
        "\n",
        "thresh = confusion_mtx.max() / 2.\n",
        "for i in range(confusion_mtx.shape[0]):\n",
        "    for j in range(confusion_mtx.shape[1]):\n",
        "        plt.text(j, i, format(confusion_mtx[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if confusion_mtx[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create classification report\n",
        "classification_rep = classification_report(y_true, y_pred, target_names=labels)\n",
        "print('Classification Report:\\n', classification_rep)"
      ],
      "metadata": {
        "id": "Gs64Kh098WEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing 1"
      ],
      "metadata": {
        "id": "YLvF783xyhiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "labels = train_generator.class_indices.keys()\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    # Load the image\n",
        "    img = image.load_img(fn, target_size=(260, 260))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    # Normalization\n",
        "    x = x / 255.0\n",
        "\n",
        "    # Predicting the image\n",
        "    proba = model.predict(x)[0]\n",
        "    maxx = proba.max()\n",
        "\n",
        "    # Displaying the image\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "    # Displaying the prediction results\n",
        "    for label, p in zip(labels, proba):\n",
        "        print(\"{}: {:.2f}%\".format(label, p * 100))\n",
        "\n",
        "    # Displaying the best prediction result\n",
        "    for label, p in zip(labels, proba):\n",
        "        if p <= 0.4:\n",
        "            continue\n",
        "        elif p == maxx:\n",
        "            if p >= 0.5:\n",
        "                print('\\nResult: ')\n",
        "                print(\"{}: {:.2f}%\".format(label, p * 100))\n",
        "            else:\n",
        "                print('\\nTanah ini tidak terdeteksi!')"
      ],
      "metadata": {
        "id": "dNDbmEDAyls1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xception"
      ],
      "metadata": {
        "id": "oCPQp_07Et7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\\\n",
        "    -O /content/Xception_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ],
      "metadata": {
        "id": "L1EGtPzPEwu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.xception import Xception\n",
        "\n",
        "# Create an instance of the inception model from the local pre-trained weights\n",
        "local_weights_file = '/content/Xception_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ],
      "metadata": {
        "id": "7QZynWttE7mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pre_trained_model(local_weights_file):\n",
        "  pre_trained_model = tf.keras.applications.xception.Xception(input_shape = (260, 260, 3),\n",
        "                                  include_top = False,\n",
        "                                  weights = None)\n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  # Make all the layers in the pre-trained model non-trainable\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  return pre_trained_model"
      ],
      "metadata": {
        "id": "k-NXqCdbFA8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model = create_pre_trained_model(local_weights_file)\n",
        "\n",
        "# Print the model summary\n",
        "pre_trained_model.summary()"
      ],
      "metadata": {
        "id": "lsNKKRu_FFib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output_of_last_layer(pre_trained_model):\n",
        "  last_desired_layer = pre_trained_model.get_layer('add_7')\n",
        "  print('last layer output shape: ', last_desired_layer.output_shape)\n",
        "  last_output = last_desired_layer.output\n",
        "  print('last layer output: ', last_output)\n",
        "\n",
        "  return last_output"
      ],
      "metadata": {
        "id": "9CA7h_OSFNi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_output = output_of_last_layer(pre_trained_model)"
      ],
      "metadata": {
        "id": "v2gIlPNfGDx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the output layer to 1 dimension\n",
        "x = keras.layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 256 hidden units and ReLU activation\n",
        "x = keras.layers.Dense(256, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = keras.layers.Dropout(0.2)(x)\n",
        "# Add a final sigmoid layer for classification\n",
        "x = keras.layers.Dense  (4, activation='softmax')(x)\n",
        "\n",
        "# Append the dense network to the base model\n",
        "model = Model(pre_trained_model.input, x)\n",
        "\n",
        "# Print the model summary. See your dense network connected at the end.\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(0.0001),  # Low learning rate\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "fYZWD7QRFaaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = myCallback()\n",
        "steps_per_epoch = 1222 // 20\n",
        "validation_steps = 341 // 20\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=100,\n",
        "                    verbose = 2,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=validation_steps,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "Lj-bN_mgFfjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the model results\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ivSNRfbUFi8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load the validation dataset\n",
        "labels = validation_generator.class_indices.keys()\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,  # Path to test directory\n",
        "    target_size=(260, 260),\n",
        "    batch_size=20,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_true = test_generator.classes\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "confusion_mtx = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot the confusion matrix as an image\n",
        "plt.imshow(confusion_mtx, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(labels))\n",
        "plt.xticks(tick_marks, labels, rotation=45)\n",
        "plt.yticks(tick_marks, labels)\n",
        "\n",
        "thresh = confusion_mtx.max() / 2.\n",
        "for i in range(confusion_mtx.shape[0]):\n",
        "    for j in range(confusion_mtx.shape[1]):\n",
        "        plt.text(j, i, format(confusion_mtx[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if confusion_mtx[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create classification report\n",
        "classification_rep = classification_report(y_true, y_pred, target_names=labels)\n",
        "print('Classification Report:\\n', classification_rep)"
      ],
      "metadata": {
        "id": "g4m7DN6aFljZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_history():\n",
        "  import pickle\n",
        "  from google.colab import files\n",
        "\n",
        "  with open('history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "  files.download('history.pkl')\n",
        "\n",
        "download_history()"
      ],
      "metadata": {
        "id": "JIew7G97OmNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Model"
      ],
      "metadata": {
        "id": "h_bB2Z01nYEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_path = \"./model_1.h5\"\n",
        "\n",
        "model.save(saved_model_path)"
      ],
      "metadata": {
        "id": "QosEQNHyOny7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('model_1.h5')"
      ],
      "metadata": {
        "id": "qMH0cb7RPPcl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}